{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GX3AhKLNP-0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------\n",
        "# IITB EdTech Internship 2025 – Problem ID 7\n",
        "# STEP 3 & 4: Saliency Maps + Comparative Metrics\n",
        "# --------------------------------------------------------------\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 0. Mount & Imports\n",
        "# --------------------------------------------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd, numpy as np, os, cv2, logging, json\n",
        "from pathlib import Path\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.stats import gaussian_kde\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 1. CONFIG\n",
        "# --------------------------------------------------------------\n",
        "CLEAN_ROOT = Path(\"/content/results/cleaned\")\n",
        "METRICS_ROOT = CLEAN_ROOT / \"metrics\"\n",
        "METRICS_ROOT.mkdir(exist_ok=True)\n",
        "\n",
        "FIX_CANVAS = CLEAN_ROOT / \"fixations_canvas.csv\"\n",
        "FDM_NATIVE_DIR = CLEAN_ROOT / \"fdm_native\"\n",
        "AVG_FDM_DIR = CLEAN_ROOT / \"fdm_group_avg\"\n",
        "\n",
        "LOG_OUT = METRICS_ROOT / \"03_step3_4.log\"\n",
        "logging.basicConfig(filename=LOG_OUT, level=logging.INFO,\n",
        "                    format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "log = logging.getLogger()\n",
        "log.info(\"=== STEP 3 & 4 STARTED ===\")\n",
        "print(\"STEP 3 & 4 – Saliency Maps + Metrics\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 2. Load Data\n",
        "# --------------------------------------------------------------\n",
        "fix_canvas = pd.read_csv(FIX_CANVAS)\n",
        "fdm_files = list(FDM_NATIVE_DIR.glob(\"*.npy\"))\n",
        "avg_fdm_files = list(AVG_FDM_DIR.glob(\"fdm_avg_*.npy\"))\n",
        "\n",
        "log.info(f\"Loaded {len(fix_canvas)} canvas fixations, {len(fdm_files)} native FDMs\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 3.1 Empirical FDMs (Gaussian-weighted) – Per Stimulus\n",
        "# --------------------------------------------------------------\n",
        "print(\"3.1 Building Empirical FDMs (Gaussian σ=visual angle)...\")\n",
        "\n",
        "CANVAS_W, CANVAS_H = 1024, 1024\n",
        "SIGMA_PX = 40  # ~1° visual angle at 1024px canvas\n",
        "\n",
        "fdm_canvas_list = []\n",
        "for (pid, qid), grp in tqdm(fix_canvas.groupby(['pid','qid']), desc=\"FDM Canvas\"):\n",
        "    canvas = np.zeros((CANVAS_H, CANVAS_W))\n",
        "    for _, row in grp.iterrows():\n",
        "        x, y = int(row['x_canvas']), int(row['y_canvas'])\n",
        "        dur = row['duration']\n",
        "        if 0 <= x < CANVAS_W and 0 <= y < CANVAS_H:\n",
        "            cv2.circle(canvas, (x, y), 1, dur, -1)\n",
        "    canvas = gaussian_filter(canvas, sigma=SIGMA_PX)\n",
        "    if canvas.sum() > 0:\n",
        "        canvas /= canvas.sum()\n",
        "    fdm_canvas_list.append({\n",
        "        'pid': pid, 'qid': qid, 'difficulty': grp['difficulty'].iloc[0],\n",
        "        'fdm': canvas\n",
        "    })\n",
        "\n",
        "FDM_CANVAS_DIR = METRICS_ROOT / \"fdm_canvas\"\n",
        "FDM_CANVAS_DIR.mkdir(exist_ok=True)\n",
        "for item in fdm_canvas_list:\n",
        "    np.save(FDM_CANVAS_DIR / f\"P{item['pid']:02d}_Q{item['qid']}.npy\", item['fdm'])\n",
        "\n",
        "log.info(f\"SAVED {len(fdm_canvas_list)} canvas FDMs → {FDM_CANVAS_DIR}\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 3.2 Temporal Maps (Early vs Late)\n",
        "# --------------------------------------------------------------\n",
        "print(\"3.2 Temporal Maps (Early vs Late)...\")\n",
        "\n",
        "temporal_maps = []\n",
        "for item in fdm_canvas_list:\n",
        "    pid, qid = item['pid'], item['qid']\n",
        "    grp = fix_canvas[(fix_canvas['pid']==pid) & (fix_canvas['qid']==qid)].copy()\n",
        "    if len(grp) < 4: continue\n",
        "    grp = grp.sort_values('start')\n",
        "    mid = len(grp) // 2\n",
        "    early = grp.iloc[:mid]\n",
        "    late = grp.iloc[mid:]\n",
        "\n",
        "    def make_map(sub):\n",
        "        m = np.zeros((CANVAS_H, CANVAS_W))\n",
        "        for _, r in sub.iterrows():\n",
        "            x, y = int(r['x_canvas']), int(r['y_canvas'])\n",
        "            if 0 <= x < CANVAS_W and 0 <= y < CANVAS_H:\n",
        "                cv2.circle(m, (x, y), 1, r['duration'], -1)\n",
        "        m = gaussian_filter(m, sigma=SIGMA_PX)\n",
        "        if m.sum() > 0: m /= m.sum()\n",
        "        return m\n",
        "\n",
        "    early_map = make_map(early)\n",
        "    late_map = make_map(late)\n",
        "    temporal_maps.append({\n",
        "        'pid': pid, 'qid': qid, 'difficulty': item['difficulty'],\n",
        "        'early': early_map, 'late': late_map\n",
        "    })\n",
        "\n",
        "TEMP_DIR = METRICS_ROOT / \"temporal\"\n",
        "TEMP_DIR.mkdir(exist_ok=True)\n",
        "for t in temporal_maps:\n",
        "    np.save(TEMP_DIR / f\"P{t['pid']:02d}_Q{t['qid']}_early.npy\", t['early'])\n",
        "    np.save(TEMP_DIR / f\"P{t['pid']:02d}_Q{t['qid']}_late.npy\", t['late'])\n",
        "\n",
        "log.info(f\"SAVED {len(temporal_maps)} temporal maps\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 4.1 Load Group-Averaged FDMs\n",
        "# --------------------------------------------------------------\n",
        "print(\"4.1 Loading Group-Averaged FDMs...\")\n",
        "avg_fdms = {}\n",
        "for f in avg_fdm_files:\n",
        "    diff = f.stem.split('_')[-1]\n",
        "    mat = np.load(f)\n",
        "    # Resize to 1024x1024\n",
        "    h, w = mat.shape\n",
        "    target_h, target_w = 1024, 1024\n",
        "    mat_resized = cv2.resize(mat, (target_w, target_h))\n",
        "    avg_fdms[diff] = mat_resized / mat_resized.sum()\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 4.2 Metric Functions\n",
        "# --------------------------------------------------------------\n",
        "from scipy.stats import wasserstein_distance\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "def similarity(m1, m2):\n",
        "    return np.sum(np.sqrt(m1 * m2))\n",
        "\n",
        "def kl_divergence(p, q):\n",
        "    p, q = p.ravel() + 1e-12, q.ravel() + 1e-12\n",
        "    return np.sum(p * np.log(p / q))\n",
        "\n",
        "def nss(saliency, fixations):\n",
        "    sal_flat = (saliency - saliency.mean()) / (saliency.std() + 1e-8)\n",
        "    return sal_flat[fixations[:,1].astype(int), fixations[:,0].astype(int)].mean()\n",
        "\n",
        "def cc(m1, m2):\n",
        "    return np.corrcoef(m1.ravel(), m2.ravel())[0,1]\n",
        "\n",
        "def auc_judd(saliency, fixations, jitter=True):\n",
        "    saliency_flat = saliency.ravel()\n",
        "    labels = np.zeros(saliency.size, dtype=int)\n",
        "    labels[fixations[:,1].astype(int) * CANVAS_W + fixations[:,0].astype(int)] = 1\n",
        "    if jitter:\n",
        "        saliency_flat += np.random.rand(*saliency_flat.shape) * 1e-8\n",
        "    return roc_auc_score(labels, saliency_flat)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 4.3 Compute Comparisons: Easy vs Medium vs Hard\n",
        "# --------------------------------------------------------------\n",
        "print(\"4.3 Computing Comparative Metrics...\")\n",
        "\n",
        "metrics_df = []\n",
        "pairs = [('easy','medium'), ('easy','hard'), ('medium','hard')]\n",
        "\n",
        "for diff1, diff2 in pairs:\n",
        "    m1 = avg_fdms[diff1]\n",
        "    m2 = avg_fdms[diff2]\n",
        "    metrics_df.append({\n",
        "        'comparison': f\"{diff1}_vs_{diff2}\",\n",
        "        'SIM': similarity(m1, m2),\n",
        "        'KL': kl_divergence(m1, m2),\n",
        "        'JS': jensenshannon(m1.ravel(), m2.ravel()),\n",
        "        'CC': cc(m1, m2),\n",
        "        'EMD': wasserstein_distance(m1.ravel(), m2.ravel())\n",
        "    })\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_df)\n",
        "metrics_df.to_csv(METRICS_ROOT / \"group_comparison_metrics.csv\", index=False)\n",
        "log.info(\"SAVED group comparison metrics\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 4.4 Point/Scanpath-Level Metrics (NSS, AUC-Judd)\n",
        "# --------------------------------------------------------------\n",
        "print(\"4.4 Point/Scanpath-Level Metrics...\")\n",
        "\n",
        "point_metrics = []\n",
        "for item in fdm_canvas_list:\n",
        "    pid, qid, diff = item['pid'], item['qid'], item['difficulty']\n",
        "    fdm = item['fdm']\n",
        "    fix = fix_canvas[(fix_canvas['pid']==pid) & (fix_canvas['qid']==qid)]\n",
        "    pts = fix[['x_canvas','y_canvas']].values.astype(int)\n",
        "    if len(pts) == 0: continue\n",
        "    point_metrics.append({\n",
        "        'pid': pid, 'qid': qid, 'difficulty': diff,\n",
        "        'NSS': nss(fdm, pts),\n",
        "        'AUC_Judd': auc_judd(fdm, pts)\n",
        "    })\n",
        "\n",
        "point_df = pd.DataFrame(point_metrics)\n",
        "point_df.to_csv(METRICS_ROOT / \"point_level_metrics.csv\", index=False)\n",
        "\n",
        "# Aggregate\n",
        "agg = point_df.groupby('difficulty').mean()[['NSS','AUC_Judd']]\n",
        "agg.to_csv(METRICS_ROOT / \"point_metrics_by_difficulty.csv\")\n",
        "log.info(\"SAVED point-level metrics\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 4.5 Statistical Testing\n",
        "# --------------------------------------------------------------\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "stats = []\n",
        "for metric in ['NSS', 'AUC_Judd']:\n",
        "    easy = point_df[point_df['difficulty']=='easy'][metric]\n",
        "    hard = point_df[point_df['difficulty']=='hard'][metric]\n",
        "    t, p = ttest_ind(easy, hard)\n",
        "    stats.append({'metric': metric, 't_stat': t, 'p_value': p})\n",
        "\n",
        "stats_df = pd.DataFrame(stats)\n",
        "stats_df.to_csv(METRICS_ROOT / \"statistical_tests.csv\", index=False)\n",
        "log.info(f\"Stats: NSS p={stats[0]['p_value']:.4f}, AUC p={stats[1]['p_value']:.4f}\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 5. Visualizations\n",
        "# --------------------------------------------------------------\n",
        "print(\"5. Visualizing Results...\")\n",
        "\n",
        "VIS_DIR = METRICS_ROOT / \"visualizations\"\n",
        "VIS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Group FDMs\n",
        "plt.figure(figsize=(15,5))\n",
        "for i, diff in enumerate(['easy','medium','hard']):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    sns.heatmap(avg_fdms[diff], cmap='viridis', cbar=True)\n",
        "    plt.title(f\"Avg FDM – {diff.capitalize()}\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(VIS_DIR / \"group_avg_fdms.png\")\n",
        "plt.close()\n",
        "\n",
        "# Difference Heatmaps\n",
        "for diff1, diff2 in pairs:\n",
        "    diff_map = avg_fdms[diff1] - avg_fdms[diff2]\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(diff_map, cmap='coolwarm', center=0, cbar=True)\n",
        "    plt.title(f\"FDM Diff: {diff1} - {diff2}\")\n",
        "    plt.savefig(VIS_DIR / f\"diff_{diff1}_vs_{diff2}.png\")\n",
        "    plt.close()\n",
        "\n",
        "# Metric Bar Plots\n",
        "fig, ax = plt.subplots(1,2, figsize=(12,5))\n",
        "sns.barplot(data=point_df, x='difficulty', y='NSS', ax=ax[0], order=['easy','medium','hard'])\n",
        "ax[0].set_title(\"NSS by Difficulty\")\n",
        "sns.barplot(data=point_df, x='difficulty', y='AUC_Judd', ax=ax[1], order=['easy','medium','hard'])\n",
        "ax[1].set_title(\"AUC-Judd by Difficulty\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(VIS_DIR / \"point_metrics_bar.png\")\n",
        "plt.close()\n",
        "\n",
        "log.info(f\"Visualizations saved → {VIS_DIR}\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# FINAL SUMMARY\n",
        "# --------------------------------------------------------------\n",
        "print(\"\\nSTEP 3 & 4 COMPLETE!\")\n",
        "print(f\"Outputs → {METRICS_ROOT}\")\n",
        "print(f\"   • fdm_canvas/                  : {len(fdm_canvas_list)} Gaussian FDMs\")\n",
        "print(f\"   • temporal/                    : early/late maps\")\n",
        "print(f\"   • group_comparison_metrics.csv : SIM, KL, CC, EMD\")\n",
        "print(f\"   • point_level_metrics.csv      : NSS, AUC-Judd per trial\")\n",
        "print(f\"   • visualizations/              : heatmaps, diffs, bars\")\n",
        "\n",
        "log.info(\"STEP 3 & 4 FINISHED\")"
      ],
      "metadata": {
        "id": "CWeukJf2LROp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}